# ğŸ• Local AI Restaurant Review Chatbot ğŸ¤–

A local AI-powered chatbot that answers questions about a pizza restaurant using customer reviews and vector similarity search. The system uses Ollama for language models and embeddings, LangChain for orchestration, and ChromaDB for vector storage.

## âœ¨ Features

- **ğŸ¤” Question Answering**: Ask natural language questions about the restaurant
- **ğŸ” Vector Search**: Uses semantic similarity to find relevant reviews
- **ğŸ  Local AI**: Runs entirely locally using Ollama models
- **ğŸ’¾ Persistent Storage**: Vector database persists between sessions
- **ğŸ’» Interactive CLI**: Simple command-line interface for queries

## ğŸ—ï¸ Architecture

The project consists of two main components:

### 1. ğŸ—ƒï¸ Vector Database Setup (`vector.py`)
- ğŸ“Š Loads restaurant reviews from CSV file
- ğŸ§  Creates vector embeddings using Ollama's `mxbai-embed-large` model
- ğŸ’¾ Stores embeddings in ChromaDB for fast similarity search
- ğŸ”„ Provides a retriever interface for finding relevant reviews

### 2. ğŸ’¬ Chat Interface (`main.py`)
- ğŸ¦™ Uses Ollama's `llama3.2:1b` model for question answering
- ğŸ” Retrieves relevant reviews based on user questions
- ğŸ“ Combines reviews with questions using a prompt template
- ğŸ–¥ï¸ Provides interactive CLI for continuous questioning

## ğŸ“‹ Prerequisites

1. **ğŸ¦™ Ollama Installation**: Install Ollama from [ollama.ai](https://ollama.ai)
2. **ğŸ¤– Required Models**: Pull the necessary models:
   ```bash
   ollama pull llama3.2:1b
   ollama pull mxbai-embed-large
   ```

## âš™ï¸ Installation

1. **ğŸ“¥ Clone or download this repository**

2. **ğŸ Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **ğŸ“„ Ensure you have the restaurant reviews data**:
   - The project includes `realistic_restaurant_reviews.csv` with sample restaurant reviews
   - Each review contains: Title, Date, Rating (1-5), and Review text

## ğŸš€ Usage

### ğŸ†• First Run
On the first run, the system will:
- ğŸ“– Read all reviews from the CSV file
- ğŸ§  Generate vector embeddings for each review
- ğŸ’¾ Store them in the ChromaDB database (`chroma_langchain_db/`)

### ğŸƒâ€â™‚ï¸ Running the Chatbot
```bash
python main.py
```

### ğŸ’¡ Example Interactions
```
Ask your question (q to quit): What do customers say about the pizza crust? ğŸ•

Ask your question (q to quit): Are there any complaints about delivery? ğŸšš

Ask your question (q to quit): What's the most expensive item mentioned? ğŸ’°

Ask your question (q to quit): q
```

## ğŸ“ Project Structure

```
Local AI Agent/
â”œâ”€â”€ ğŸ main.py                          # Main chat interface
â”œâ”€â”€ ğŸ” vector.py                        # Vector database setup and retriever
â”œâ”€â”€ ğŸ“‹ requirements.txt                 # Python dependencies
â”œâ”€â”€ ğŸ“„ realistic_restaurant_reviews.csv # Restaurant review data
â”œâ”€â”€ ğŸ“– README.md                        # This file
â””â”€â”€ ğŸ—ƒï¸ chroma_langchain_db/            # ChromaDB vector database (auto-generated)
    â”œâ”€â”€ ğŸ’¾ chroma.sqlite3
    â””â”€â”€ ğŸ“‚ b686ea07-51e6-47e4-aba5-25fadda50e30/
        â”œâ”€â”€ ğŸ“Š data_level0.bin
        â”œâ”€â”€ ğŸ“‹ header.bin
        â”œâ”€â”€ ğŸ“ length.bin
        â””â”€â”€ ğŸ”— link_lists.bin
```

## âš¡ How It Works

1. **ğŸ“Š Data Loading**: Reviews are loaded from the CSV file with metadata (rating, date)
2. **ğŸ§  Embedding Generation**: Each review is converted to a vector using Ollama embeddings
3. **ğŸ’¾ Vector Storage**: Embeddings are stored in ChromaDB for fast retrieval
4. **ğŸ” Query Processing**: User questions are embedded and matched against stored reviews
5. **ğŸ¯ Context Retrieval**: Top 5 most relevant reviews are retrieved
6. **ğŸ’¬ Answer Generation**: The language model generates answers using the relevant reviews as context

## ğŸ“¦ Dependencies

- **ğŸ”— langchain**: Framework for building LLM applications
- **ğŸ¦™ langchain-ollama**: Ollama integration for LangChain
- **ğŸ—ƒï¸ langchain-chroma**: ChromaDB integration for LangChain
- **ğŸ¼ pandas**: Data manipulation for CSV processing

## âš™ï¸ Configuration

### ğŸ”„ Changing Models
To use different Ollama models, modify the model names in:
- `main.py`: Change `llama3.2:1b` to your preferred language model
- `vector.py`: Change `mxbai-embed-large` to your preferred embedding model

### ğŸ›ï¸ Adjusting Retrieval
To change the number of reviews retrieved per query, modify the `k` parameter in `vector.py`:
```python
retriever = vector_store.as_retriever(search_kwargs={"k": 5})  # Change 5 to desired number
```

### ğŸ“Š Custom Data
Replace `realistic_restaurant_reviews.csv` with your own data. Ensure the CSV has columns:
- `Title`: Review title
- `Date`: Review date  
- `Rating`: Numerical rating
- `Review`: Review text content

## âš¡ Performance Notes

- â±ï¸ First run will be slower due to embedding generation
- ğŸš€ Subsequent runs use the persisted vector database
- ğŸƒâ€â™‚ï¸ Query response time depends on the Ollama model speed
- ğŸ’¨ The `llama3.2:1b` model is chosen for fast local inference

## ğŸ”§ Troubleshooting

1. **ğŸ”Œ Ollama Connection Issues**: Ensure Ollama is running (`ollama serve`)
2. **ğŸ¤– Model Not Found**: Pull required models using `ollama pull <model-name>`
3. **ğŸ’¾ Memory Issues**: Consider using smaller models if you have limited RAM
4. **ğŸ—ƒï¸ Database Issues**: Delete `chroma_langchain_db/` folder to rebuild the vector database
